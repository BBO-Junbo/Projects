{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer:\n",
    "    def __init__(self):\n",
    "        self.string = \"\"\n",
    "        self.pointer = 0\n",
    "        self.keywords = {\n",
    "            \"int\": (5, \"-\"),\n",
    "            \"else\": (15, \"-\"),\n",
    "            \"if\": (17, \"-\"),\n",
    "            \"while\": (20, \"-\")\n",
    "        }\n",
    "\n",
    "        self.constants = {\n",
    "            \"整数\": (100, \"整数\")\n",
    "        }\n",
    "\n",
    "        self.operators = {\n",
    "            \"+\": (41, \"-\"),\n",
    "            \"-\": (42, \"-\"),\n",
    "            \"*\": (43, \"-\"),\n",
    "            \"/\": (44, \"-\"),\n",
    "            \"%\": (45, \"-\"),\n",
    "            \"=\": (46, \"-\"),\n",
    "            \">\": (47, \"-\"),\n",
    "            \">=\": (48, \"-\"),\n",
    "            \"<\": (49, \"-\"),\n",
    "            \"<=\": (50, \"-\"),\n",
    "            \"==\": (51, \"-\"),\n",
    "            \"!=\": (52, \"-\"),\n",
    "            \"&&\": (53, \"-\"),\n",
    "            \"||\": (54, \"-\"),\n",
    "            \"!\": (55, \"-\"),\n",
    "            \"++\": (56, \"-\"),\n",
    "            \"--\": (57, \"-\")\n",
    "        }\n",
    "\n",
    "        self.delimiters = {\n",
    "            \"(\": (81, \"-\"),\n",
    "            \")\": (82, \"-\"),\n",
    "            \",\": (83, \"-\"),\n",
    "            \";\": (84, \"-\"),\n",
    "            \"{\": (86, \"-\"),\n",
    "            \"}\": (87, \"-\"),\n",
    "            \"[\": (88, \"-\"),\n",
    "            \"]\": (89, \"-\")\n",
    "        }\n",
    "        \n",
    "    def count_tokens(self):\n",
    "        tokens = []\n",
    "        current_token = \"\"\n",
    "        i = 0\n",
    "        while i < len(self.string):\n",
    "            if self.string[i].isdigit():  # Check if the character is a digit\n",
    "                current_token += self.string[i]\n",
    "                i += 1\n",
    "                while i < len(self.string) and self.string[i].isdigit():\n",
    "                    current_token += self.string[i]\n",
    "                    i += 1\n",
    "                tokens.append((100, current_token))  # Append the Integer constant with token type 100\n",
    "                current_token = \"\"\n",
    "            elif self.string[i].isalnum() or self.string[i] == '_':\n",
    "                current_token += self.string[i]\n",
    "                i += 1\n",
    "                while i < len(self.string) and (self.string[i].isalnum() or self.string[i] == '_'):\n",
    "                    current_token += self.string[i]\n",
    "                    i += 1\n",
    "                if current_token in self.keywords:\n",
    "                    tokens.append(self.keywords[current_token])\n",
    "                else:\n",
    "                    tokens.append(self.is_identifier(current_token))\n",
    "                current_token = \"\"\n",
    "            elif self.string[i] in self.operators or self.string[i:i+2] in self.operators:\n",
    "                if self.string[i:i+2] in self.operators:\n",
    "                    tokens.append(self.operators[self.string[i:i+2]])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    tokens.append(self.operators[self.string[i]])\n",
    "                    i += 1\n",
    "            elif self.string[i] in self.delimiters:\n",
    "                tokens.append(self.delimiters[self.string[i]])\n",
    "                i += 1\n",
    "            elif self.string[i].isspace():\n",
    "                i += 1\n",
    "            else:\n",
    "                raise ValueError(\"Illegal character: {}\".format(self.string[i]))\n",
    "        return len(tokens)\n",
    "    \n",
    "    def is_identifier(self, token):\n",
    "        return 111, token\n",
    "\n",
    "    def get_next_token(self):\n",
    "        char = self.string[self.pointer]\n",
    "        token=\"\"\n",
    "        if char.isdigit():  # Check if the character is a digit\n",
    "            token = char\n",
    "            self.pointer += 1\n",
    "            while self.pointer < len(self.string) and self.string[self.pointer].isdigit():\n",
    "                token += self.string[self.pointer]\n",
    "                self.pointer += 1\n",
    "            return (100, token)  # Return the Integer constant with token type 100\n",
    "        elif char.isalnum() or char == '_':\n",
    "            token = char\n",
    "            self.pointer += 1\n",
    "            while self.pointer < len(self.string) and (self.string[self.pointer].isalnum() or self.string[self.pointer] == '_'):\n",
    "                token += self.string[self.pointer]\n",
    "                self.pointer += 1\n",
    "            if token in self.keywords:\n",
    "                return self.keywords[token]\n",
    "            else:\n",
    "                return self.is_identifier(token)\n",
    "        elif char in self.operators or (self.string[self.pointer:self.pointer + 2]) in self.operators:\n",
    "            if (self.string[self.pointer:self.pointer + 2]) in self.operators:\n",
    "                self.pointer += 2\n",
    "                return self.operators[self.string[self.pointer-2:self.pointer]]\n",
    "            else:\n",
    "                self.pointer += 1\n",
    "                return self.operators[char]\n",
    "        elif char in self.delimiters:\n",
    "            self.pointer += 1\n",
    "            return self.delimiters[char]\n",
    "        elif char.isspace():\n",
    "            self.pointer += 1\n",
    "            return self.get_next_token()\n",
    "\n",
    "    def analyze(self):\n",
    "        if self.pointer >= len(self.string):\n",
    "            return None\n",
    "        return self.get_next_token()\n",
    "\n",
    "    def analyze_file(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_content = file.read()  # Read file contents once\n",
    "                self.string = file_content  # Assign the content to self.string\n",
    "                self.token_num = self.count_tokens()\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) bool ⟶ equality\n",
      "(2) equality ⟶ rel rest4\n",
      "(3) rel ⟶ expr rop_expr\n",
      "(4) expr ⟶ term rest5\n",
      "(5) term ⟶ unary rest6\n",
      "(6) unary ⟶ factor\n",
      "(7) factor ⟶ num\n",
      "(8) rest6 ⟶ ℇ\n",
      "(9) rest5 ⟶ ℇ\n",
      "(10) rop_expr ⟶ ℇ\n",
      "(11) rest4 ⟶ ℇ\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Parser:\n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        self.current_token = None\n",
    "        self.step = 1\n",
    "\n",
    "    def eat(self, token_type):\n",
    "        if self.current_token is not None and self.current_token[0] == token_type:\n",
    "            self.current_token = self.lexer.analyze()\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected token\")\n",
    "\n",
    "    def print_step(self, production):\n",
    "        print(f\"({self.step}) {production}\")\n",
    "        self.step += 1\n",
    "\n",
    "    def factor(self):\n",
    "        if self.current_token is None:\n",
    "            return\n",
    "        self.print_step(\"factor ⟶ num\")\n",
    "        token_type = self.current_token[0]\n",
    "        if token_type == 100:\n",
    "            token_value = self.current_token[1]\n",
    "            self.eat(100)\n",
    "            return int(token_value)\n",
    "        elif token_type == 111:\n",
    "            identifier = self.current_token[1]\n",
    "            self.eat(111)\n",
    "            return identifier\n",
    "        elif self.current_token[1] == '(':\n",
    "            self.eat(81)\n",
    "            result = self.expression()\n",
    "            self.eat(82)\n",
    "            return result\n",
    "\n",
    "    def unary(self):\n",
    "        if self.current_token is None:\n",
    "            return\n",
    "        self.print_step(\"unary ⟶ factor\")\n",
    "        return self.factor()\n",
    "\n",
    "    def term(self):\n",
    "        if self.current_token is None:\n",
    "            return\n",
    "        self.print_step(\"term ⟶ unary rest6\")\n",
    "        result = self.unary()\n",
    "        while self.current_token is not None and self.current_token[0] in (43, 44):\n",
    "            op = self.current_token[0]\n",
    "            self.eat(op)\n",
    "            if op == 43:\n",
    "                self.print_step(\"rest6 ⟶ * unary rest6\")\n",
    "                result *= self.unary()\n",
    "            elif op == 44:\n",
    "                self.print_step(\"rest6 ⟶ / unary rest6\")\n",
    "                result /= self.unary()\n",
    "        self.print_step(\"rest6 ⟶ ℇ\")\n",
    "        return result\n",
    "\n",
    "    def expression(self):\n",
    "        if self.current_token is None:\n",
    "            return\n",
    "        self.print_step(\"expr ⟶ term rest5\")\n",
    "        result = self.term()\n",
    "        while self.current_token is not None and self.current_token[0] in (41, 42):\n",
    "            op = self.current_token[0]\n",
    "            self.eat(op)\n",
    "            if op == 41:\n",
    "                self.print_step(\"rest5 ⟶ + term rest5\")\n",
    "                result += self.term()\n",
    "            elif op == 42:\n",
    "                self.print_step(\"rest5 ⟶ - term rest5\")\n",
    "                result -= self.term()\n",
    "        self.print_step(\"rest5 ⟶ ℇ\")\n",
    "        return result\n",
    "\n",
    "    def rop_expr(self):\n",
    "        if self.current_token is None:\n",
    "            return None\n",
    "        while self.current_token is not None and self.current_token[0] in (47, 48, 49, 50):\n",
    "            op = self.current_token[0]\n",
    "            self.eat(op)\n",
    "            if op == 49:\n",
    "                self.print_step(\"rop_expr ⟶ < expr\")\n",
    "            elif op == 50:\n",
    "                self.print_step(\"rop_expr ⟶ <= expr\")\n",
    "            elif op == 47:\n",
    "                self.print_step(\"rop_expr ⟶ > expr\")\n",
    "            elif op == 48:\n",
    "                self.print_step(\"rop_expr ⟶ >= expr\")\n",
    "            result = self.expression()\n",
    "            return result\n",
    "        else:\n",
    "            self.print_step(\"rop_expr ⟶ ℇ\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def rel(self):\n",
    "        if self.current_token is None:\n",
    "            return\n",
    "        self.print_step(\"rel ⟶ expr rop_expr\")\n",
    "        left = self.expression()\n",
    "        right = self.rop_expr()\n",
    "        if right is not None:\n",
    "            return left, right\n",
    "        return left\n",
    "\n",
    "    def equality(self):\n",
    "        if self.current_token is None:\n",
    "            return None\n",
    "        self.print_step(\"equality ⟶ rel rest4\")\n",
    "        left = self.rel()\n",
    "\n",
    "        while self.current_token is not None and self.current_token[0] in (51, 52):\n",
    "            op = self.current_token[0]\n",
    "            self.eat(op)\n",
    "\n",
    "            if op == 51:\n",
    "                self.print_step(\"rest4 ⟶ ==rel rest4\")\n",
    "            elif op == 52:\n",
    "                self.print_step(\"rest4 ⟶ !=rel rest4\")\n",
    "\n",
    "            right = self.rel()\n",
    "            left = (left, op, right)\n",
    "\n",
    "        self.print_step(\"rest4 ⟶ ℇ\")\n",
    "        return left\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def bool_expr(self):\n",
    "        if self.current_token is None:\n",
    "            return\n",
    "        self.print_step(\"bool ⟶ equality\")\n",
    "        return self.equality()\n",
    "\n",
    "    def parse(self):\n",
    "        self.current_token = self.lexer.analyze()\n",
    "        return self.bool_expr()\n",
    "\n",
    "lexer = Lexer()\n",
    "lexer.analyze_file(\"source_code2.txt\")\n",
    "parser = Parser(lexer)\n",
    "result = parser.parse()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aodprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
